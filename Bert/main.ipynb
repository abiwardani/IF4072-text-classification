{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\rifky\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from unicodedata import name\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import collections\n",
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "from sklearn import preprocessing\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "from tqdm import tqdm\n",
    "from nltk.tokenize import word_tokenize\n",
    "from torch.utils.data import TensorDataset, random_split\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "nltk.download('punkt')\n",
    "\n",
    "# running program using external model and do testing\n",
    "\n",
    "\n",
    "def run(model_path, test_dataset_path, stopwords_path):\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "        print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "    else:\n",
    "        print('No GPU available, using the CPU instead.')\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "    # Load a trained model and vocabulary that you have fine-tuned\n",
    "    model = BertForSequenceClassification.from_pretrained(model_path)\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "\n",
    "    # Copy the model to the GPU.\n",
    "    model.to(device)\n",
    "\n",
    "    # Reading test file\n",
    "    test = pd.read_csv(test_dataset_path)\n",
    "    test = data_cleansing(test, stopwords_path)\n",
    "    run_test(test, model, tokenizer, device)\n",
    "\n",
    "\n",
    "def run_test(test, model, tokenizer, device):\n",
    "    # Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    test_sentences = test[\"text_cleansing\"]\n",
    "    test_labels = test[\"label\"]\n",
    "\n",
    "    # For every sentence...\n",
    "    for sent in test_sentences:\n",
    "        # `encode_plus` will:\n",
    "        #   (1) Tokenize the sentence.\n",
    "        #   (2) Prepend the `[CLS]` token to the start.\n",
    "        #   (3) Append the `[SEP]` token to the end.\n",
    "        #   (4) Map tokens to their IDs.\n",
    "        #   (5) Pad or truncate the sentence to `max_length`\n",
    "        #   (6) Create attention masks for [PAD] tokens.\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "            sent,                      # Sentence to encode.\n",
    "            add_special_tokens=True,  # Add '[CLS]' and '[SEP]'\n",
    "            max_length=256,           # Pad & truncate all sentences.\n",
    "            pad_to_max_length=True,\n",
    "            return_attention_mask=True,   # Construct attn. masks.\n",
    "            return_tensors='pt',     # Return pytorch tensors.\n",
    "        )\n",
    "\n",
    "        # Add the encoded sentence to the list.\n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "\n",
    "        # And its attention mask (simply differentiates padding from non-padding).\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "    # Convert the lists into tensors.\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "    labels = torch.tensor(test_labels)\n",
    "\n",
    "    # Set the batch size.\n",
    "    batch_size = 32\n",
    "\n",
    "    # Create the DataLoader.\n",
    "    prediction_data = TensorDataset(input_ids, attention_masks, labels)\n",
    "    prediction_sampler = SequentialSampler(prediction_data)\n",
    "    prediction_dataloader = DataLoader(\n",
    "        prediction_data, sampler=prediction_sampler, batch_size=batch_size)\n",
    "\n",
    "    # Prediction on test set\n",
    "\n",
    "    print('Predicting labels for {:,} test sentences...'.format(\n",
    "        len(input_ids)))\n",
    "\n",
    "    # Put model in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables\n",
    "    predictions, true_labels = [], []\n",
    "\n",
    "    # Predict\n",
    "    for batch in prediction_dataloader:\n",
    "        # Add batch to GPU\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        # Unpack the inputs from our dataloader\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "        # Telling the model not to compute or store gradients, saving memory and\n",
    "        # speeding up prediction\n",
    "        with torch.no_grad():\n",
    "            # Forward pass, calculate logit predictions\n",
    "            outputs = model(b_input_ids, token_type_ids=None,\n",
    "                            attention_mask=b_input_mask)\n",
    "\n",
    "        logits = outputs[0]\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        # Store predictions and true labels\n",
    "        predictions.append(logits)\n",
    "        true_labels.append(label_ids)\n",
    "\n",
    "    print('    DONE.')\n",
    "    # Calculate accuracy for test dataset\n",
    "    total_accuracy = 0\n",
    "    for batch_num in range(len(predictions)):\n",
    "        total_accuracy += flat_accuracy(\n",
    "            predictions[batch_num], true_labels[batch_num])\n",
    "\n",
    "    total_accuracy = total_accuracy/len(predictions)\n",
    "    print('Accuracy on test dataset: {}'.format(total_accuracy))\n",
    "\n",
    "\n",
    "def get_frequent_word(df):\n",
    "    text = \" \".join(list(df['text_a'].str.lower()))\n",
    "    word_list = word_tokenize(text)\n",
    "    word_count = dict(collections.Counter(word_list))\n",
    "    d_word_freq = pd.DataFrame(\n",
    "        data={'word': list(word_count.keys()), 'freq': list(word_count.values())})\n",
    "\n",
    "    return d_word_freq\n",
    "\n",
    "\n",
    "def cleansing(text, stopword=None):\n",
    "    word_list = word_tokenize(text.lower())\n",
    "    word_list = [word for word in word_list if len(word) > 2]\n",
    "    word_list = [word for word in word_list if word.isalnum()]\n",
    "    if stopword == None:\n",
    "        text = ' '.join(word_list)\n",
    "    else:\n",
    "        word_list = [word for word in word_list if word not in stopword]\n",
    "        text = ' '.join(word_list)\n",
    "\n",
    "    return text\n",
    "\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "def data_cleansing(test, stopwords_path):\n",
    "    stopwords = list(pd.read_csv(stopwords_path))\n",
    "\n",
    "    for i in tqdm(range(len(test))):\n",
    "        test.loc[i, 'text_cleansing'] = cleansing(\n",
    "            test.loc[i, 'text_a'], stopword=stopwords)\n",
    "\n",
    "    # dict mapping\n",
    "    labels = [\"no\", \"yes\"]\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(labels)\n",
    "    test[\"label\"] = le.transform(test[\"label\"])\n",
    "\n",
    "    return test\n",
    "\n",
    "\n",
    "if name == \"__main__\":\n",
    "    run(\"model\", \"datasets/test.csv\", \"datasets/stopwords.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, using the CPU instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 2800/2800 [00:09<00:00, 298.76it/s]\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "c:\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2302: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting labels for 2,800 test sentences...\n",
      "    DONE.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'flat_accuracy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-384834c5da47>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"model\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"datasets/test.csv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"datasets/stopwords.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-6-e4ce3e9fca52>\u001b[0m in \u001b[0;36mrun\u001b[1;34m(model_path, test_dataset_path, stopwords_path)\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_dataset_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_cleansing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstopwords_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m     \u001b[0mrun_test\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-e4ce3e9fca52>\u001b[0m in \u001b[0;36mrun_test\u001b[1;34m(test, model, tokenizer, device)\u001b[0m\n\u001b[0;32m    127\u001b[0m     \u001b[0mtotal_accuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mbatch_num\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 129\u001b[1;33m         total_accuracy += flat_accuracy(\n\u001b[0m\u001b[0;32m    130\u001b[0m             predictions[batch_num], true_labels[batch_num])\n\u001b[0;32m    131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'flat_accuracy' is not defined"
     ]
    }
   ],
   "source": [
    "run(\"model\", \"datasets/test.csv\", \"datasets/stopwords.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
