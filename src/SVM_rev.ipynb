{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import wordnet as wn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "np.random.seed(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../dataset/train.csv',encoding='latin-1')\n",
    "dftest = pd.read_csv('../dataset/test.csv',encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['text_a'].dropna(inplace=True)\n",
    "df['text_a'] = [entry.lower() for entry in df['text_a']]\n",
    "df['text_a']= [word_tokenize(entry) for entry in df['text_a']]\n",
    "tag_map = defaultdict(lambda : wn.NOUN)\n",
    "tag_map['J'] = wn.ADJ\n",
    "tag_map['V'] = wn.VERB\n",
    "tag_map['R'] = wn.ADV\n",
    "for index,entry in enumerate(df['text_a']):\n",
    "    Final_words = []\n",
    "    word_Lemmatized = WordNetLemmatizer()\n",
    "    for word, tag in pos_tag(entry):\n",
    "        if word not in stopwords.words('english') and word.isalpha():\n",
    "            word_Final = word_Lemmatized.lemmatize(word,tag_map[tag[0]])\n",
    "            Final_words.append(word_Final)\n",
    "            df.loc[index,'text_final'] = str(Final_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_a'].dropna(inplace=True)\n",
    "df['text_a'] = [entry.lower() for entry in df['text_a']]\n",
    "df['text_a']= [word_tokenize(entry) for entry in df['text_a']]\n",
    "tag_map = defaultdict(lambda : wn.NOUN)\n",
    "tag_map['J'] = wn.ADJ\n",
    "tag_map['V'] = wn.VERB\n",
    "tag_map['R'] = wn.ADV\n",
    "for index,entry in enumerate(df['text_a']):\n",
    "    Final_words = []\n",
    "    word_Lemmatized = WordNetLemmatizer()\n",
    "    for word, tag in pos_tag(entry):\n",
    "        if word not in stopwords.words('english') and word.isalpha():\n",
    "            word_Final = word_Lemmatized.lemmatize(word,tag_map[tag[0]])\n",
    "            Final_words.append(word_Final)\n",
    "            df.loc[index,'text_final'] = str(Final_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 4969)\t0.08544674302521513\n",
      "  (0, 4752)\t0.192253804338474\n",
      "  (0, 4372)\t0.17257556928740986\n",
      "  (0, 3997)\t0.1327193061594158\n",
      "  (0, 3343)\t0.15554130577029882\n",
      "  (0, 3055)\t0.4784343351816939\n",
      "  (0, 2504)\t0.16368094277055012\n",
      "  (0, 2187)\t0.22038203667081135\n",
      "  (0, 1996)\t0.1201091535581707\n",
      "  (0, 1640)\t0.6015118621285316\n",
      "  (0, 903)\t0.19431786116896668\n",
      "  (0, 872)\t0.058821637981268424\n",
      "  (0, 736)\t0.15944991974512454\n",
      "  (0, 541)\t0.1425792713159156\n",
      "  (0, 394)\t0.1440985123614311\n",
      "  (0, 338)\t0.21933908496287946\n",
      "  (0, 324)\t0.1903283826577389\n",
      "  (0, 151)\t0.10758347435701064\n",
      "  (1, 4724)\t0.36310916612811556\n",
      "  (1, 3683)\t0.46639757059391834\n",
      "  (1, 2993)\t0.4864171528031431\n",
      "  (1, 2637)\t0.3831812955502047\n",
      "  (1, 2558)\t0.4385310053947065\n",
      "  (1, 872)\t0.2736531474781427\n",
      "  (2, 4746)\t0.2965085097045991\n",
      "  :\t:\n",
      "  (21598, 2164)\t0.2204807560884628\n",
      "  (21598, 981)\t0.16894464074661952\n",
      "  (21598, 897)\t0.28210619995748437\n",
      "  (21598, 872)\t0.10183155564432896\n",
      "  (21598, 626)\t0.252649214965177\n",
      "  (21598, 455)\t0.33640154833185626\n",
      "  (21599, 4762)\t0.19339419794125864\n",
      "  (21599, 4676)\t0.33542993892264134\n",
      "  (21599, 4651)\t0.26045877110074717\n",
      "  (21599, 2661)\t0.27444757403750697\n",
      "  (21599, 1077)\t0.6191994702370691\n",
      "  (21599, 981)\t0.15372489116960814\n",
      "  (21599, 919)\t0.2320616424584683\n",
      "  (21599, 872)\t0.09265783596257457\n",
      "  (21599, 852)\t0.12245852403265288\n",
      "  (21599, 370)\t0.3110810014086261\n",
      "  (21599, 220)\t0.21916403977476426\n",
      "  (21599, 101)\t0.2786528942785593\n",
      "  (21600, 3830)\t0.3403337744595632\n",
      "  (21600, 3666)\t0.40549674211119086\n",
      "  (21600, 981)\t0.1624208101996004\n",
      "  (21600, 965)\t0.36176217405817324\n",
      "  (21600, 440)\t0.31866828693291344\n",
      "  (21600, 261)\t0.31866828693291344\n",
      "  (21600, 162)\t0.5994947620626986\n"
     ]
    }
   ],
   "source": [
    "X = df['text_a']\n",
    "Y = df['label']\n",
    "Xtest = dftest['text_a']\n",
    "Ytest = dftest['label']\n",
    "\n",
    "Encoder = LabelEncoder()\n",
    "Y = Encoder.fit_transform(Y)\n",
    "Ytest = Encoder.fit_transform(Ytest)\n",
    "\n",
    "Tfidf_vect = TfidfVectorizer(max_features=5000)\n",
    "Tfidf_vect.fit(df['text_a'])\n",
    "Train_X_Tfidf = Tfidf_vect.transform(X)\n",
    "Test_X_Tfidf = Tfidf_vect.transform(Xtest)\n",
    "\n",
    "print(Train_X_Tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy Score ->  86.42857142857143\n"
     ]
    }
   ],
   "source": [
    "Naive = naive_bayes.MultinomialNB()\n",
    "Naive.fit(Train_X_Tfidf,Y)\n",
    "predictions_NB = Naive.predict(Test_X_Tfidf)\n",
    "print(\"Naive Bayes Accuracy Score -> \",accuracy_score(predictions_NB, Ytest)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [21601, 2800]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [34], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m SVM \u001b[38;5;241m=\u001b[39m svm\u001b[38;5;241m.\u001b[39mSVC(C\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m, kernel\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlinear\u001b[39m\u001b[38;5;124m'\u001b[39m, degree\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mSVM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTrain_X_Tfidf\u001b[49m\u001b[43m,\u001b[49m\u001b[43mYtest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m predictions_SVM \u001b[38;5;241m=\u001b[39m SVM\u001b[38;5;241m.\u001b[39mpredict(Test_X_Tfidf)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSVM Accuracy Score -> \u001b[39m\u001b[38;5;124m\"\u001b[39m,accuracy_score(predictions_SVM, Ytest\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\NITRO 5\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:173\u001b[0m, in \u001b[0;36mBaseLibSVM.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    171\u001b[0m     check_consistent_length(X, y)\n\u001b[0;32m    172\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 173\u001b[0m     X, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[0;32m    174\u001b[0m         X,\n\u001b[0;32m    175\u001b[0m         y,\n\u001b[0;32m    176\u001b[0m         dtype\u001b[39m=\u001b[39;49mnp\u001b[39m.\u001b[39;49mfloat64,\n\u001b[0;32m    177\u001b[0m         order\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mC\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    178\u001b[0m         accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    179\u001b[0m         accept_large_sparse\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    180\u001b[0m     )\n\u001b[0;32m    182\u001b[0m y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_targets(y)\n\u001b[0;32m    184\u001b[0m sample_weight \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(\n\u001b[0;32m    185\u001b[0m     [] \u001b[39mif\u001b[39;00m sample_weight \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m sample_weight, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mfloat64\n\u001b[0;32m    186\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\NITRO 5\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:596\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    594\u001b[0m         y \u001b[39m=\u001b[39m check_array(y, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_y_params)\n\u001b[0;32m    595\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 596\u001b[0m         X, y \u001b[39m=\u001b[39m check_X_y(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_params)\n\u001b[0;32m    597\u001b[0m     out \u001b[39m=\u001b[39m X, y\n\u001b[0;32m    599\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m check_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mensure_2d\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n",
      "File \u001b[1;32mc:\\Users\\NITRO 5\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:1092\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1074\u001b[0m X \u001b[39m=\u001b[39m check_array(\n\u001b[0;32m   1075\u001b[0m     X,\n\u001b[0;32m   1076\u001b[0m     accept_sparse\u001b[39m=\u001b[39maccept_sparse,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1087\u001b[0m     input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mX\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1088\u001b[0m )\n\u001b[0;32m   1090\u001b[0m y \u001b[39m=\u001b[39m _check_y(y, multi_output\u001b[39m=\u001b[39mmulti_output, y_numeric\u001b[39m=\u001b[39my_numeric, estimator\u001b[39m=\u001b[39mestimator)\n\u001b[1;32m-> 1092\u001b[0m check_consistent_length(X, y)\n\u001b[0;32m   1094\u001b[0m \u001b[39mreturn\u001b[39;00m X, y\n",
      "File \u001b[1;32mc:\\Users\\NITRO 5\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:387\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    385\u001b[0m uniques \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(lengths)\n\u001b[0;32m    386\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(uniques) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m--> 387\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    388\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    389\u001b[0m         \u001b[39m%\u001b[39m [\u001b[39mint\u001b[39m(l) \u001b[39mfor\u001b[39;00m l \u001b[39min\u001b[39;00m lengths]\n\u001b[0;32m    390\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [21601, 2800]"
     ]
    }
   ],
   "source": [
    "SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "SVM.fit(Train_X_Tfidf,Y)\n",
    "predictions_SVM = SVM.predict(Test_X_Tfidf)\n",
    "print(\"SVM Accuracy Score -> \",accuracy_score(predictions_SVM, Y*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "200e2a1a991d180c9c309c789f657d054eb1e69794afbc0e2ae3f183f1fccf4c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
